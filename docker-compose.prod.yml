version: "3.3"
volumes:
  dataset-volume: # this is the volume where the dataset will be mounted, so that all workers can access it
services:
  # Backend API container
  api:
    depends_on: [spark-master, spark-worker-1, spark-worker-2]
    build:
      context: api
      target: prod
      args:
        dataset: ml-latest
    environment:
      # FORCE_RECOMPUTE_DATASET: "true"
      SPARK_MASTER: spark://spark-master:7077
    volumes:
      - dataset-volume:/dataset # this container actually writes the dataset in its build process (by downloading it from the internet)
  # Frontend UI container
  frontend:
    depends_on: [api]
    build:
      context: frontend
      target: prod
    environment:
      REACT_APP_API_URL: http://api
    ports:
      - "3000:3000"
  # Spark master
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      INIT_DAEMON_STEP: setup_spark
  # Spark worker #1
  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    depends_on: [spark-master]
    ports:
      - "8081:8081"
    environment:
      SPARK_MASTER: spark://spark-master:7077
    volumes:
      - dataset-volume:/dataset
  # Spark worker #2
  spark-worker-2:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-2
    depends_on: [spark-master]
    ports:
      - "8082:8081"
    environment:
      SPARK_MASTER: spark://spark-master:7077
    volumes:
      - dataset-volume:/dataset
